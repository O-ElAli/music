{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a6e5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geome\\AppData\\Local\\Temp\\ipykernel_1204\\4220505250.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\", map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "=== Generated ABC Notation ===\n",
      "\n",
      "X:1\n",
      "T:Generated Tune\n",
      "M:6/8\n",
      "L:1/8\n",
      "K:G\n",
      " e2 | e2 f2 e4 | e2 c2 BABc | dBge dcBA | G2 G2 A2 G2 :| d2 e2 efga | gfed e2 ef | \n",
      " gfga g2 g2 | e2 g2 G2 G2 | g2 gf g2 e2 | dcBA G2 D2 | G2 G2 G2 B2 | BcdB A2 A2 | \n",
      " Bcde d2 B2 | c2 BA A2 A2 | B2 G2 BABG | G2B2 d2 G2 | BdBA G4 :: D2GG BAGG | B2G2 A4 | \n",
      " BGGA B2B2 | [AA]2AB A2G2 | G2GG G2G2 :| (de/f/g)g g2 A4 | d2df a2(ba) | g4{g} f4 e2 | \n",
      " dged dBG2 | f2ef{df} e2d2 | dcBA TB2>e2 | (dBg)B (ABA2) | GBG2 (B2G2) | e2a2g2 (de) | \n",
      " d2B2 G4 || d2B2c2 e2d2>B2 | B2E2E2 c2B2g2 | e2>d'2 e'2c'2 A2A2 | E2>A2 G2>A2 B4 | \n",
      " e2>e2 E2(3cBA e2e2 | c2>A2 c2B2 A2GA B2c2 | d2>e2 c2A2 A2G2 G2DE | G2AG E2>G2 G2A2 d2BG | \n",
      " A2e2 A2d2 ^cede c2AG E2G2 | G2B2 G2>B2d2c2 Bcde d^cde | ^c2A2 ^G2A2 EAcd B2GE G4 g4 :: \n",
      " G2B2 G2>^A2B2 G2G2G2 G2B2 AG2F F2>F2 | E2>A2 G2<E2 B4 A8 A4 B2c2 | B8- B4 B8 B4 :: \n",
      "[M:6/8] B4 | A8 B2A\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "# === Parameters ===\n",
    "SEQ_LENGTH = 75\n",
    "EMBED_DIM = 64\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 1\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# === Load data to get vocabulary ===\n",
    "print(\"Loading dataset and vocab...\")\n",
    "dataset = load_dataset('sander-wood/irishman', split='train')\n",
    "texts = dataset['abc notation']\n",
    "vocab = sorted(set(''.join(texts)))\n",
    "char2idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "idx2char = {i: ch for ch, i in char2idx.items()}\n",
    "VocabSize = len(vocab)\n",
    "\n",
    "# === Define model ===\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(VocabSize, EMBED_DIM)\n",
    "        self.lstm = nn.LSTM(EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_DIM, VocabSize)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = out.reshape(-1, HIDDEN_DIM)\n",
    "        logits = self.fc(out)\n",
    "        return logits, hidden\n",
    "\n",
    "# === Load model ===\n",
    "model = SimpleRNN().to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", map_location=DEVICE))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# === Generate ABC tune ===\n",
    "M_options = [\"M:6/8\", \"M:4/4\", \"M:3/4\"]\n",
    "L_options = [\"L:1/8\", \"L:1/16\", \"L:1/4\"]\n",
    "K_options = [\"K:D\", \"K:G\", \"K:C\"]\n",
    "\n",
    "start = f\"\"\"X:1\n",
    "T:Generated Tune\n",
    "{random.choice(M_options)}\n",
    "{random.choice(L_options)}\n",
    "{random.choice(K_options)}\n",
    "\"\"\"\n",
    "\n",
    "seq = [char2idx.get(c, 0) for c in start]\n",
    "hidden = None\n",
    "\n",
    "for _ in range(800):  # generate 800 characters\n",
    "    inp_seq = seq[-SEQ_LENGTH:]\n",
    "    inp = torch.tensor(inp_seq).unsqueeze(0).to(DEVICE)\n",
    "    logits, hidden = model(inp, hidden)\n",
    "    probs = torch.softmax(logits[-1], dim=0)\n",
    "    idx = torch.multinomial(probs, 1).item()\n",
    "    seq.append(idx)\n",
    "\n",
    "# === Decode and print ===\n",
    "generated = ''.join(idx2char[i] for i in seq)\n",
    "print(\"\\n=== Generated ABC Notation ===\\n\")\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10851e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
