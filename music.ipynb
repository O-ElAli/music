{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e88550",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "In this assignment, you will implement a Recurrent Neural Network (RNN) for music generation.  \n",
    "\n",
    "For this, you will use the Irish Massive ABC Notation (IrishMAN) dataset, which contains a collection of Irish folk tunes in ABC notation.  \n",
    "\n",
    "The goal is to train an RNN to generate new tunes based on the patterns learned from the dataset.\n",
    "\n",
    "**Dataset:**  \n",
    "IrishMAN Dataset can be found at [https://huggingface.co/datasets/sander-wood/irishman](https://huggingface.co/datasets/sander-wood/irishman).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56028bee",
   "metadata": {},
   "source": [
    "**Tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a362fe",
   "metadata": {},
   "source": [
    "a) Data Preparation: Download the IrishMAN dataset and preprocess the ABC notation files to create a suitable input format for the RNN.  \n",
    "\n",
    "This includes tokenizing the ABC notation and creating sequences of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae3a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\geome\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# dataset = load_dataset(\"sander-wood/irishman\")\n",
    "\n",
    "# train_dataset = dataset[\"train\"]\n",
    "# validation_dataset = dataset[\"validation\"]\n",
    "\n",
    "\n",
    "# # print(\"Train dataset size:\", len(train_dataset))\n",
    "\n",
    "# # print(\"Validation dataset size:\", len(validation_dataset))\n",
    "\n",
    "# train_data = \"\\n\".join(i['abc notation'] for i in train_dataset)\n",
    "\n",
    "# # numberOfSequences = 10\n",
    "\n",
    "# # sequenceLength = 100\n",
    "# # # Create a list to hold the sequences\n",
    "\n",
    "\n",
    "# # # Initialize the sequence list with None\n",
    "\n",
    "\n",
    "# # sequence = [None] * numberOfSequences\n",
    "\n",
    "# # # In the loop below, you can extract sequences of length 100 from the concatenated train_data string:\n",
    "# # for i in range(numberOfSequences):\n",
    "# #     start_idx = i * sequenceLength\n",
    "# #     end_idx = start_idx + sequenceLength\n",
    "# #     sequence[i] = train_data[start_idx:end_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75290258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b81747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data on cuda...\n",
      "Using 50000 tunes for training\n",
      "Vocab size: 95\n",
      "Epoch 1/20, Loss: 1.1400\n",
      "Validation Loss: 1.1254\n",
      "Epoch 2/20, Loss: 1.1043\n",
      "Validation Loss: 1.1183\n",
      "Epoch 3/20, Loss: 1.0973\n",
      "Validation Loss: 1.1120\n",
      "Epoch 4/20, Loss: 1.0936\n",
      "Validation Loss: 1.1099\n",
      "Epoch 5/20, Loss: 1.0912\n",
      "Validation Loss: 1.1093\n",
      "Epoch 6/20, Loss: 1.0895\n",
      "Validation Loss: 1.1077\n",
      "Epoch 7/20, Loss: 1.0882\n",
      "Validation Loss: 1.1065\n",
      "Epoch 8/20, Loss: 1.0871\n",
      "Validation Loss: 1.1061\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "# 0. Control the data size for quick testing\n",
    "SAMPLE_SIZE = 50000     # number of tunes to use (set to None for full dataset)\n",
    "\n",
    "# 1. Set parameters\n",
    "SEQ_LENGTH = 75       # number of characters per training sequence\n",
    "BATCH_SIZE = 32       # how many sequences per batch\n",
    "EMBED_DIM = 64        # size of character embeddings\n",
    "HIDDEN_DIM = 128      # size of LSTM hidden state\n",
    "NUM_LAYERS = 1        # number of LSTM layers\n",
    "LEARNING_RATE = 0.001 # optimizer learning rate\n",
    "EPOCHS = 20            # number of passes through the data\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 2. Load data from Hugging Face\n",
    "print(f\"Loading data on {DEVICE}...\")\n",
    "dataset = load_dataset('sander-wood/irishman', split='train')\n",
    "texts = dataset['abc notation']  # list of music strings\n",
    "if SAMPLE_SIZE is not None:\n",
    "    texts = texts[:SAMPLE_SIZE]   # keep only a small subset\n",
    "print(f\"Using {len(texts)} tunes for training\")\n",
    "\n",
    "# 3. Build a character-level vocabulary\n",
    "vocab = sorted(set(''.join(texts)))\n",
    "char2idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "idx2char = {i: ch for ch, i in char2idx.items()}\n",
    "VocabSize = len(vocab)\n",
    "print(f\"Vocab size: {VocabSize}\")\n",
    "\n",
    "# 4. Create a simple Dataset class\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.data = []\n",
    "        for t in texts:\n",
    "            # turn text into list of indices\n",
    "            seq = [char2idx.get(c, 0) for c in t]\n",
    "            # slice into pairs of (input, target)\n",
    "            for i in range(len(seq) - SEQ_LENGTH):\n",
    "                inp = seq[i:i+SEQ_LENGTH]\n",
    "                tgt = seq[i+1:i+SEQ_LENGTH+1]\n",
    "                self.data.append((inp, tgt))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        inp, tgt = self.data[idx]\n",
    "        return torch.tensor(inp), torch.tensor(tgt)\n",
    "\n",
    "# 5. Split into train/val\n",
    "random.shuffle(texts)\n",
    "split_idx = int(0.9 * len(texts))\n",
    "train_texts = texts[:split_idx]\n",
    "val_texts = texts[split_idx:]\n",
    "train_ds = MusicDataset(train_texts)\n",
    "val_ds = MusicDataset(val_texts)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 6. Define the LSTM model\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(VocabSize, EMBED_DIM)\n",
    "        self.lstm = nn.LSTM(EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_DIM, VocabSize)\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = out.reshape(-1, HIDDEN_DIM)\n",
    "        logits = self.fc(out)\n",
    "        return logits, hidden\n",
    "\n",
    "model = SimpleRNN().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 7. Training loop\n",
    "# Early stopping parameters\n",
    "patience = 5              # how many epochs to wait without improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE).view(-1)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(inputs)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE).view(-1)\n",
    "            logits, _ = model(inputs)\n",
    "            val_loss += criterion(logits, targets).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Check for improvement\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Optionally save the best model here\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "            early_stop = True\n",
    "\n",
    "    if early_stop:\n",
    "        break\n",
    "# 8. Generate a sample sequence with dynamic M, L, K\n",
    "model.eval()\n",
    "# Keep X and T fixed, but choose M, L, K randomly\n",
    "M_options = [\"M:6/8\", \"M:4/4\", \"M:3/4\"]\n",
    "L_options = [\"L:1/8\", \"L:1/16\", \"L:1/4\"]\n",
    "K_options = [\"K:D\", \"K:G\", \"K:C\"]\n",
    "# Build header\n",
    "start = f\"\"\"X:1\n",
    "T:Generated Tune\n",
    "{random.choice(M_options)}\n",
    "{random.choice(L_options)}\n",
    "{random.choice(K_options)}\n",
    "\"\"\"\n",
    "# Convert header to indices\n",
    "seq = [char2idx.get(c, 0) for c in start]\n",
    "hidden = None\n",
    "# Generate additional 200 characters of music\n",
    "for _ in range(800):\n",
    "    inp_seq = seq[-SEQ_LENGTH:]\n",
    "    inp = torch.tensor(inp_seq).unsqueeze(0).to(DEVICE)\n",
    "    logits, hidden = model(inp, hidden)\n",
    "    probs = torch.softmax(logits[-1], dim=0)\n",
    "    idx = torch.multinomial(probs, 1).item()\n",
    "    seq.append(idx)\n",
    "\n",
    "print(\"Training finished in:\", datetime.now() - start_time)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, 'best_model.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert back to characters and print\n",
    "gen = ''.join(idx2char[i] for i in seq)\n",
    "print(\"Generated ABC notation with dynamic headers:\", gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3825dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(f\"Sequence 1:\\n{sequence[0]}\\n\")\n",
    "# print(f\"Length of sequence 1: {len(sequence[0])} characters\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebd805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simple_tokenizer(abc: str) -> list:\n",
    "#     \"\"\"\n",
    "#     Tokenizes a cleaned ABC notation string using a basic stateful loop.\n",
    "#     It treats symbols like ^, _ (accidentals), durations, and note groups as single tokens.\n",
    "#     \"\"\"\n",
    "#     tokens = []\n",
    "#     current_token = ''\n",
    "\n",
    "#     for char in abc:\n",
    "#         if char.isspace():\n",
    "#             if current_token:\n",
    "#                 tokens.append(current_token)\n",
    "#                 current_token = ''\n",
    "#         elif char in \"|[]:()\":\n",
    "#             if current_token:\n",
    "#                 tokens.append(current_token)\n",
    "#                 current_token = ''\n",
    "#             tokens.append(char)\n",
    "#         elif char in \"^_=,'/0123456789ABCDEFGabcdefgzxzZ\":  # acceptable musical characters\n",
    "#             current_token += char\n",
    "#         else:\n",
    "#             # Unknown character, break current token and skip\n",
    "#             if current_token:\n",
    "#                 tokens.append(current_token)\n",
    "#                 current_token = ''\n",
    "    \n",
    "#     if current_token:\n",
    "#         tokens.append(current_token)\n",
    "\n",
    "#     return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e45f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_abc_text(abc_text: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Removes ABC metadata headers and keeps only the music notation after the 'K:' line.\n",
    "#     This avoids tokenizing title, meter, tempo, etc.\n",
    "#     \"\"\"\n",
    "#     lines = abc_text.split('\\n')\n",
    "#     music_started = False\n",
    "#     music_lines = []\n",
    "\n",
    "#     for line in lines:\n",
    "#         if line.startswith('K:'):  # Start music after key signature\n",
    "#             music_started = True\n",
    "#             continue\n",
    "#         if music_started:\n",
    "#             music_lines.append(line)\n",
    "\n",
    "#     return ' '.join(music_lines).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8c9961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long token:  \n",
      "Long token:  \n",
      "Long token:  \n",
      "Long token:  \n",
      "Long token:  \n",
      "Long token:  \n",
      "Long token:  \n",
      "Long token:  \n",
      "Long token:  \n",
      "Long token:  \n",
      "Long token:  \n",
      "Long token:  \n",
      "Long token:  \n",
      "Long token:  \n",
      "Long token:  \n",
      "Long token:  \"^Slow\" A3 A2- A c2 e | d3 Td2 ^c d2 A | f3 Te2 c g2 e | d3 d2 e c2 G | A3 A2 A c2 e |   d3 (Td2 ^c) d2 A | f3 f2 A f2 f | g3 (Tg2 f) g2 g |{fg} a3 a2 g Ta2 g | f3 (f2 g){fg} a2 g |   f !fermata!e2 a Tg>ed c2 G :: A3 A2 A cde | d3 Td2 c dcA | f3 e2 c geg | d3 d2 e cGc |   A3 A2 A cde | d3 Td2 c dcA | f3 f2 A fef | g3 Tg2 f gfg | a3 Ta2 g aga | f3 f2 g agf |  {f} !fermata!e2 a ged cGc ::[M:6/8]\"^\\\"the variations follow\\\"\" A3 A2 A/A/ | c3 c2 e/e/ |   d3 d2 c/c/ | d3 d2 A/A/ | f3 e2 c/c/ | g3 Tg2 e/e/ | d3 d2 A/A/ | c3 c2 G/G/ | A3 A2 A/A/ |   c3 c2 E/E/ | [Dd]3 d2 c/c/ | d3 d2 A/A/ | f3 f2 A/A/ | f3 f2 f/f/ | g3 Tg2 f/f/ | g2 g2 f/f/ |   a3 a2 g/g/ | a3 a2 g/g/ | f3 f2 g/g/ | a3 Ta2 g/g/ |{f} e3 e2 d/d/ | c3 c2 G/G/ ::   A2 A/A/ A2 A/A/ | c2 c/c/ c2 e/e/ | d2 d/d/ d2 ^c/c/ | d2 d/d/ d2 A/A/ | f2 f/f/ e2 c/c/ |   g2 g/g/ Tg2 e/e/ | d2 d/d/ d2 A/A/ | c2 c/c/ c2 G/G/ | A2 A/A/ A2 A/A/ | c2 c/c/ c2 E/E/ |   [Dd]2 d/d/ d2 c/c/ | d2 d/d/ d2 A/A/ | f2 f/f/ f2 A/A/ | f2 f/f/ f2 f/f/ | g2 g/g/ Tg2 f/f/ |   g2 g/g/ Tg2 f/f/ | a2 a/a/ Ta2 g/g/ | a2 a/a/ Ta2 g/g/ | f2 f/f/ f2 g/g/ | a2 a/a/ Ta2 g/g/ |  {f} e2 e/e/ e2 g/g/ | d2 e/e/ c2 G/G/ :: AAA AAA | ccc cce | ddd ddc | ddd ddA | fff eec |   Tggg gge | ddd dde | ccc ccG | AAA AAA | ccc cce | ddd dd^c | ddd ddA | fff ffA | fff fff |   ggg ggf | ggg ggf | aaa aag | aaa aag | fff ffg | aaa aag | eee eea | Tg>ed cAG :: A/A/AA A2 A |   c/c/cc c2 e | d/d/dd d2 ^c | d/d/dd d2 A | f/f/ff e2 c | g/g/gg g2 e | d/d/dd d2 e | c/c/cc c2 G |   A/A/AA A2 A | c/c/cc c2 e | d/d/dd d2 ^c | d/d/dd d2 A | f/f/ff f2 A | f/f/ff f2 f |   g/g/gg Tg2 f | g/g/gg g2 f | a/a/aa Ta2 g | a/a/aa Ta2 g | f/f/ff f2 g | a/a/aa Ta2 g |   e/e/ee e2 a | Tg>ed Tc>AG :: A/A/AA A/A/AA | c/c/cc c/c/ce | d/d/dd d/d/dc | d/d/dd d/d/dA |   f/f/ff e/e/ec | g/g/gg g/g/ge | d/d/dd d/d/de | c/c/cc c/c/cG | A/A/AA A/A/AA | c/c/cc c/c/ce |   d/d/dd d/d/d^c | d/d/dd d/d/dA | f/f/ff f/f/fA | f/f/ff f/f/ff | g/g/gg g/g/gf | g/g/gg g/g/gf |   a/a/aa a/a/ag | a/a/aa a/a/ag | f/f/ff f/f/fg | a/a/aa a/a/ag | e/e/ee e/e/eg | d/d/de c/c/G :|\n",
      "Long token:  E2 | A4 E2G2 A2A,2 z2 A2 | B2c2 d2e2 dcBc A2e2 | E2e2E2c2 E2B2 z2 e2 | Eddd Eccc E2B2 z2 E2 |  \"^5\" EFGA B2c2 d2c2 z2 B2 | ABcd e2f2 =g2f2 z2 e2 | a2dc d2b2 d2cB c2a2 | c2BA d2c2 c2B2 z2 e2 |  \"^9\" ceee Beee ceee Aeee | dfff cfff dfff Bfff | Aaaa Baaa caaa Baaa | Aaaa Baaa caaa Baaa |  \"^13\" ABcd efge | agaf gfgd | e4 ^d4 e6 :: e2 |\"^16\" a2gf edcB A4 z2 E2 |  \"^17\" A2GF EDCB, A,4 z2 e2 |\"^18\" eaga eaga eaga eaga | ebab ebab ebab ebab |   ec'bc' ec'bc' ec'bc' ec'bc' |\"^21\" ed'c'd' ed'c'd' ed'c'd' ed'c'd' |   ee'e'e' e'e'e'e' fe'e'e' e'e'e'e' | ge'e'e' e'e'e'e' fe'e'e' e'e'e'e' |   ee'e'e' e'e'e'e' fe'e'e' e'e'e'e' |\"^25\" ge'e'e' e'e'e'e' fe'e'e' e'e'e'e' |   ee'e'e' fe'e'e' ge'e'e' fe'e'e' | ee'e'e' fe'e'e' ge'e'e' fe'e'e' | ee'fe' ge'fe' ee'fe' ge'fe' |  \"^29\" ee'fe' ge'fe' ee'fe' ge'fe' | ge'e'e' ee'e'e' e'e'e'e' e'e'e'e' |   ge'e'e' ee'e'e' e'e'e'e' e'e'e'e' | ge'e'e' ee'e'e' e'e'e'e' e'e'e'e' |   ge'e'e' ee'e'e' e'e'e'e' e'e'e'e' |\"^34\" ee'ee' fe'fe' ge'ge' fe'fe' |   ee'ee' fe'fe' ge'ge' fe'fe' | (e/f/g/a/ b/c'/d'/e'/) d'c'd'e' d'2c'2 z2 A2 |   a2gf g2a2 f2e2 z2 e2 |\"^38\" ceBe AeBe ceBe AeBe | ceBe AeBe ceBe AeBe | dece BeAe dece BeAe |   dece BeAe E4 z2 E2 |\"^42\" EFGA B2c2 d2c2 z2 B2 | ABcd e2f2 =g2f2 z2 c2 | e4 d4 c2E2 c2e2 |   cBAc E2G2 A2A,2 z2 ::[M:3/4][Q:1/4=140]\"^Minuet\" E4 |\"^46\" A4 A2A2A2A2 | A2B2 c2B2 c2A2 |   B2a2 g2f2 e2d2 |{d} Tc6 d2c2B2 |\"^50\" A2E2 B2E2 c2E2 | A2E2 B2E2 c2E2 | d2e2 d4 c4 |  {d} c6 d2c2B2 |\"^54\" A2E2 A2c2 B2A2 | c2A2 c2e2 d2c2 | B2E2 G2B2 d2B2 | d2B2 d2f2 d2B2 |  \"^58\" A2B2 c2d2{f} Te4 | G2A2 B2c2 d4 | B2c2 d2e2{f} e4 | B2c2 d2e2{f} e4 |\"^62\" f2g2 a2g2 f2g2 |   a2g2 f2e2 d2c2 | d2e2 d4 c4 |{c} B8 :: efg2 |\"^66\" a2A2 A2A2 A2A2 | e2A2 A2A2 A2A2 |   f2e2 d2c2 B2A2 | G2A2 B2G2 E4 |\"^70\" A2c2 e2c2 A2c2 | B2d2 f2d2 B2d2 | B2a2 g2f2 e2d2 |  {d} Tc6 d2c2B2 |\"^74\" A2a2 a2a2 a2a2 | B2a2 a2a2 a2a2 | c2a2 a2a2 a2a2 | B2a2 a2a2 a2a2 |  \"^78\" A2a2 B2a2 c2a2 | c2a2 B2a2 A2a2 | A2a2 B2a2 c2a2 | c2a2 B2a2 A2a2 |\"^82\" a2g2 f2e2 d2c2 |   f2e2 d2c2 B2A2 | G2A2 B2G2 A2B2 |{F} E12 |\"^86\" A2B2 c2d2{f} e4 | G2A2 B2c2 d4 |   B2c2 d2e2{g} Tf4 | A2B2 c2d2{f} Te4 |\"^90\" f2g2 a2g2 f2g2 | a2g2 f2e2 d2c2 | (3(d2c2B2) c4 B4 |   A8 :|\n",
      "Long token:  |: e4{f} (ed/c/) | (f3 d B2) | d2 d2{g} (fe/d/) | (g3 e c2) | (B>dc>ed>f) | e2{b} (ag) g2 |   f2 (f>de>c) | (e4 d2) | d4 (^fg) |{^f} e2 (dcBA) | (d>e) G2 ^F2 | (^F4 G2) :: (B>d) G2 G2 |   f4{e} (dc/B/) | g3 (abc') |{c'} b2 (ag^fg) | aA A3 d | (c2 B2 c2) | (^ga=gfed) | (c2 B2 c2) |   c'3 (gbg) | (c'bagfe) |{d} d2 c2 (TB3/2A/4B/4) | (B4 c2) ::\"^Variazioni 1\" (3EGc (3egf (3edc |   (fg).e (de).c .B2 | (3.G(Bd) (3fag (3fed | (3(ga).f (3(ef).d .c2 | (3BgB{d} (3(cBc) (3dgf |   (3.e(ag) (3.c(ag) (3.e(ag) | (3.f(dB) (3.d'.b.f (3.e.d.c | (3.B(^fg) (3GGG{A} (3G^FG |   (3dd'c' (3bag (3^fed | (3(^de)e (3(Bc)c (3(^GA)A | (3d(^de) (3B=dg (TA3/2G/4A/4) |   (3GBd (3gbd' G2 :: (3(GD).d (3BGD (3GBd | (3(fd).d' (3bgd (3Bcd | (3cef (3gab (3c'd'e' |   (3(d'b).g (3ggg{a} (g^fg) | (3aA^c (3def{g} (3fed | (3(ge).c (3Bg.B (3.c.e.g |   (3aA^c (3def{g} (3fed | (3(ge).c (3BgB (3.c.e.g | (3c'(e'd') (3c'ba (3gab | (3c'ba (3gfe (3dc_B |   (3Afd c2 TB3/2A/4B/4 | (d4 c2) ::\"^Variazioni 2\" .c'.g z (a/g/) (f/e/d/c/) |   (B/c/d/e/) (f/g/a/g/) .f z | .d'.g z (B/c/) (d/e/f/g/) | (e/g/f/e/) (f/g/a/b/) .c' z |   g>B g>c g>d | g>e .c'/.b/.a/.g/ .f/.e/.d/.c/ | ff z (d/B/) (c/G/c/e/) |   (d/g/^f/g/) .a/(g/f/g/) G2 | .dd' z (c'/b/) (a/g/^f/g/) | .e.e' z (d'/c'/) (b/a/^g/a/) |   =g/e/d/c/ B/c/d/B/ TA3/2(G/4A/4) | G(G/D/) (B/G/)(d/B/) g z :: .g.d' z (G/A/) (B/c/d/e/) |   .f.d' z (f/e/) (d/c/B/d/) | .c.c' z (c'/b/) (a/g/f/e/) | (d/e/c/d/) B/c/A/B/ G z |   a/_b/a/g/ (f/e/d/^c/) (d/f/e/d/) | .=c.g{g} fe/f/ e2 | (a/_b/a/g/) (f/e/d/^c/) (d/f/e/d/) |   .=cg{g} fe/f/ e2 | .c'g z (d'/c'/) (b/g/a/b/) | .c'/(g/^f/g/) (f/g/=f/g/) (e/g/e/c/) |   .A.a z (f/d/) (e/c/d/B/) | c3/2(B/4c/4) .e/.c/.g/.e/ c' z ::  \"^Variazioni 3\" .e(e/d/) .c(c/B/) .A(A/G/) | (F3 E D2) | .f(f/e/) .d(d/c/) .B(B/A/) | (G3 F E2) |   .D(d/B/) .c(e/c/) .f(f/d/) | .gg/e/ .c'c'/g/ .ee/c/ | .f(f/d/) .B(B/d/) .c(c/e/) | (g3 e) d2 |   (d/c/B/c/) (d/e/^f/g/) (a/b/c'/d'/) | e'3 | (d'>c') b2 Tag/a/ | (a4 g2) ::   .G(G/D/) .B(B/G/) .d(d/B/) | (f3 e d2) | .c(c/G/) .e(e/c/) .g(g/e/) | (d3 g) G2 |   .A(^c/d/) .A(e/f/) .A(^g/a/) | .=g(g/c'/) .b(b/g/) c'c | .A(^c/d/) .A(e/f/) .A(^g/a/) |   .=g(g/c'/) .b(b/g/) c'c | c'/b/a/g/ f/e/d/c/ B/G/A/B/ |   (c/c'/)(B/b/) (c/c'/)(d/d'/) (e/e'/)(g/_b/) | (a/g/f/e/) .c(c/e/){e} dc/B/ | (B4 c2) :|\n",
      "Long token:  (uc>B) |{AB} c2E2E2F2 (A2>B2)A2c2 | (B2>c2) dcBA{AB} c2F2F2 (c>B) |{AB} c2E2E2F2 (A2>B2)A2 ag |   f2>e2 fgae c2A2A2 (cB) |{AB} c2E2E2F2 (A2>B2) (Ac)ec | Bdce dcBA{AB} c2F2F2 (c>B) |  {AB} c2E2E2F2 (A2>B2) (Ae)ae | (f3{gf}e) (dc)Be c2A2A2e2 | (f3/2{gf}e)fg a2c2 (d>c)de f2A2 |   B>ABc dcBA{AB} c2F2F2 (c>B) |{AB} c2E2E2(FG) (A2>B2) (Ae)ag | (f3{gf}e) (fg)ae c2A2A2 |] (uc>B) |   A2E2C2A,2 A2E2c2E2 | (BE)(cE) dcBA{AB} c2F2F2 (c>B) | A2E2C2A,2 e2A2f2A2 |   a(g/f/)ed (c>e)(B>e) c2A2A2 (ef/g/) | aece fece aece fece | agfe dcBA{AB} c2F2F2 (c>B) |   AECE FECE AECE FECE | a2(g/f/)ed (c>e)(B>e) c2A2A2 |] (uc>B) | A2(c>A) E2(A>E) C2(E>C) A,2c2 |   BABc dcBA{AB} c2F2F2 (c>B) | A2(3(ecA) E2(3(cAE) C2(3(AEC) A,2(ag) | f2>e2 fgae c2A2A2e2 |   f2af e2ac d2bd c2aA | (.B>.d)(.c>.e) (.d>.c)B>A{AB} c2F2F2 (c>B) |   A2(3(ecA) D2(3(cAE) C2(3(AEC) A,2(ag) | (f3{gf}e) (fg)ae c2A2A2 |] (uc>B) |  {AB} c2E2E2F2 (A2>B2)A2c2 |{c} (BA)Bc dcBA{AB} c2F2F2 (c>B) |{AB} c2E2(E2F2) (A2>B2)A2 (a/g/f/e/) |   (f/e/d/c/ d/c/B/A/) (G/A/B/A/ G/F/E/D/) C2A,2A,2e2 | fdf(g/a/) e2>c2 dbB(c/d/) c2>A2 |  {c} BABc dcBA{AB} c2F2F2 (c>B) |{AB} c2E2(E2F2) (A2>B2)A2 (a/g/f/e/) |   (f/e/d/c/ d/c/B/A/) (G/A/B/A/ G/F/E/D/) C2A,2 A,2 |] (uc>B) ||   (3AcB (3AGF (3EAE (3FAF (3EDC (3DCB, (3CB,A, (3ABc | (3B^AB (3cBc (3dcB (3cB=A{AB} c2F2F2 (c>B) |   (3AcB (3AGF (3EAE (3FAF (3EDC (3DCB, (3CB,A, (3ecA | (3fag (3fed (3cBA (3Bed c2A2A2 (ef/g/) |   (3aec (3aec (3aec (3fec (3aec (3aec (3aec (3fec | (3agf (3efg (3agf (3edc{de} f2F2F2 (c>B) |   (3AEC (3AEC (3AEC (3FEC (3AEC (3AEC (3AEC (3FEC | (3AGF (3EFG (3AGF (3EFD C2A,2A,2 |] (uc>B) |   A2E2 E(F/G/ B/A/G/F/) E2(F/G/) AB cd^de | cd z uB dcBA{AB} c2F2F2 (c>B) |   A2E2 E(F/G/ B/A/G/F/) E(F/G/ A/B/c/d/) eAag | (f/g/a/g/ f/e/d/c/ B).e.^d.=d c2A2A2 (cd/e/) |   (f2>a2)e2c2 (d2>b2)c2 (3(aec) | Bdce dBcA{AB} c2F2F2 (c>B) |   A2E2 E(F/G/ B/A/G/F/) E(F/G/ A/B/c/d/ .e).A.a.g | (f/g/a/g/ f/e/d/c/ B).e.^d.=d c2A2A2 |] (uc>B) |   A2(.E>.E) E2F2 E2(.A>.A) A2c2 | BEBc dcBA{AB} c2(.F>.F) F2(c>B) |   A2(.E>.E) E2F2 E2(.A>.A) A2(.a>.a) | f2(.d>.d) B2(.e>.e) c2(.A>.A) A2(ef/g/) |   a2(.a>.a) (.a>.g)(.f>.e) a2(.a>.a) (.a>.g)(.f>.e) |   (.a>.g)(.f>.e) (.d>.c)(.B>.A){AB} c2(.F>.F) F2(c>B) | A2(.A>.A) A2(F>E) A2(.A>.A) A2(.a>.a) |   f2(.d>.d) B2(.e>.e) c2(.A>.A)\"_Fine.\" A2 |]\n",
      "Long token:  !ff! D2DDD2 z2 | F2FFF2 z2 | A2AAA2 z2 |  \"^ad lib\" !fermata!d4- d2^cded cded =c2A2F2!fermata!D2 ^cded cded =c2A2F2!fermata!D2z2D2 F2>F2 A2>A2 d2>d2 f2>f2 a2>a2 !fermata!d'4 D2F2A2d2 !fermata!Td4 D2F2A2d2 !fermata!Td4 ^cdfe =cAEF !fermata!TB2(A/G/A/B/^c/d/e/f/) .g2.f2.e2.d2 =c2B2A2G2 !fermata!F4 (D/E/F/G/A/B/^c/d/ e/f/g/a/b/^c'/d'/) d2 !fermata!d4 ||  \"^Tempo di polka\" (3.d.d.d | dd^cd edBG | D6 (3.d.d.d | dd^cd edBG | D6 (3.=c.c.c |   =ccBA e2 (3.B.B.B | BBAG d2>d2 | ^cccc cgfe | d2^c2 d2(3.d.d.d | dd^cd edBG | D6 (3.d.d.d |   dd^cd edBG | E6 (3.c.c.c | ccBA e2 (3.B.B.B | BBAG d2>d2 | ^cGAB =cdef | g2g2g2 z2 || dd^cc eedd |   BBGG D2 z2 | dd^cc eedd | BBGG E2 z2 | ccAA ffee | BBGG eedd | cABc defg | gdBG D2 z2 |   dd^cc eedd | BBGG D2 z2 | dd^cc eedd | BBGG E2 z2 | ccAA ffee | BBGG eedd | cABc defg |   g2!>!f2!>!e2!>!d2 | dd^cc eedd | BdBd D2 z2 | DEFG ABcd | e2e2e2 z2 | ee^dd ffee | ccAA eedd |   DEFG ABcd | edBG D2 z2 | dd^cc eedd | BdBd D2 z2 | DEFG ABcd | e2e2e2 z2 | ggff eedd |   eedd ^ccdd | eedd eeff | g2g2g2 z2 ||\"_-5,16 3\"\"_-6,10 ...\" d2\"_-5,12 3\"\"_-6,6 ...\" ^c2 e2d2 |  \"_-5,15 3\"\"_-6,9 ...\" B2\"_-6,-7 ...\"\"_-4,-20 3\" A2 c2B2 | BABA GAGF | E2e2e2 z2 |  \"_-5,16 3\"\"_-6,10 ...\" d2\"_-5,16 3\"\"_-6,10 ...\" ^d2 f2e2 |  \"_-5,12 3\"\"_-6,6 ...\" c2\"_-6,-7 ...\"\"_-4,-20 3\" A2 e2d2 | DEFG ABcd | edBG D2 z2 |  \"_-5,16 3\"\"_-6,10 ...\" d2\"_-5,12 3\"\"_-6,6 ...\" ^c2 e2d2 |  \"_-5,15 3\"\"_-6,9 ...\" B2\"_-6,-7 ...\"\"_-4,-20 3\" A2 c2B2 | BABA GAGF | EAce g2 z2 |  \"_-5,12 3\"\"_-6,6 ...\" g2 gf ed^cd | ed^cd e2d2 |\"_-5,12 3\"\"_-6,6 ...\" e2 de fdef |   g2!>!f2!>!e2!>!d2 |  \"_-5,16 3\"\"_-6,10 ...\" d2\"_-5,12 3\"\"_-6,6 ...\" ^c2\"_-5,12 3\"\"_-6,6 ...\" e2\"_-5,16 3\"\"_-6,10 ...\" d2 |   BdBd D2 z2 | DEFG ABcd | e2e2e2 z2 |\"_-5,12 3\"\"_-6,6 ...\" e2\"_-5,16 3\"\"_-6,10 ...\" ^d2 f2e2 |  \"_-5,12 3\"\"_-6,6 ...\" c2\"_-6,-7 ...\"\"_-4,-20 3\" A2 e2d2 | DEFG ABcd | edBG D2 z2 |  \"_-5,16 3\"\"_-6,10 ...\" d2\"_-5,12 3\"\"_-6,6 ...\" ^c2\"_-5,12 3\"\"_-6,6 ...\" e2\"_-5,16 3\"\"_-6,10 ...\" d2 |   BdBd D2 z2 | DEFG ABcd | e2e2e2 z2 |  \"_-5,12 3\"\"_-6,6 ...\" g2\"_-5,12 3\"\"_-6,6 ...\" f2\"_-5,12 3\"\"_-6,6 ...\" e2\"_-5,16 3\"\"_-6,10 ...\" d2 |  \"_-5,12 3\"\"_-6,6 ...\" e2\"_-5,16 3\"\"_-6,10 ...\" d2\"_-5,12 3\"\"_-6,6 ...\" ^c2\"_-5,16 3\"\"_-6,10 ...\" d2 |  \"_-5,12 3\"\"_-6,6 ...\" e2\"_-5,16 3\"\"_-6,10 ...\" d2\"_-5,12 3\"\"_-6,6 ...\" e2\"_-5,12 3\"\"_-6,6 ...\" f2 |   g2g2g2 z2 || z4 z2\"_-6,-7 ...\"\"_-4,-20 3\" D2 |: G2(3DDD B2(3DDD | d6 ^cd | ed^cd ed=cA |   F4 z2 (3DDD | F2(3DDD A2(3DDD | c6 AF | DEFG ABcd |1 (e4 d2) (3DDD :|2 (e4 d2) z2 ||  \"_-5,16 3\"\"_-6,10 ...\" d2\"_-5,12 3\"\"_-6,6 ...\" ^c2 e2d2 |  \"_-5,15 3\"\"_-6,9 ...\" B2\"_-6,-7 ...\"\"_-4,-20 3\" A2 c2B2 | BABA GAGF |   EAce !fermata!g2\"_-5,16 3\"\"_-6,10 ...\" d2 | g2(3BBB d2(3GGG | B2(3DDD G2FE | D^CDE FDEF |  \"^presto\" G4 B4 | d4 g2\"_-5,16 3\"\"_-6,10 ...\" d2 | g2(3ddd e2(3ddd | g2(3ddd e2(3ddd |   g2(3ddd e2(3ddd | g2(3ddd e2(3ddd | g4 z2 b2 | b4 z2 g2 | !fermata!g8 |]\n",
      "Long token:  F^EF || GzFB z F^EF | GzFc z F^EF | GFec ^AFE^D | D3 ^A B^A=A^G | GzBe z GBe | FzBd z FB^B |   c3 d cB^AB | [cf]3 [cf]2 F^EF | [B,G]zF[B,B] z F^EF | [Ge]zF[ce] z F^EF | GFec ^AFE^D |   D3 ^A B^A=A^G | GBeG BeGB | FBdF Bd (3F^Ac | fFfg fedc | [Bf]3 [Bf]2 F^EF | GF^EF BFEF |   GF^EF cFEF | GFec ^AFE^D | D3 B B^A=A^G | GBeG BeGB | FBdF BdFd | c3 d cB^AB | c2 (3^ABc fF^EF |   [B,G]zF[B,B] z F^EF | [Ge]zF[ce] z F^EF | GFec ^AFE^D | D3 B{c} B^A=A^G | GFGB eGBe |   FBdF Bd (3F^Ac | fFfg fedc | [Bf]3 [Bf]2 FG^G || [Af]z^G[Af] z G[Af]^A | [Bg] z [Ge]2 z FG^G |   ACEA CEc^B | B2 [FA]2 z ddA | dAdA cAcA | B2 G2 G=C^CE | czGB z FG^G | A z [Fd]2 z ^EFG |   ADFA DFA^A | BEGB Ed (3c^B=B | ACEA CEA^A | BF F2 FAdc | B^ABc edcd | fga^a b=agf | edcB AGFE |   [Dd]3 D DFA^A | [Af]z^G[Af] z G[Af]^A | [Bg] z [Ge]2 z FG^G | ACEA CEc^B | B2 [FA]2 z [Ad][Ad]A |   dAdA cAcA | B2 G2 G=C^CE | czGB z FG^G | A z [Fd]2 z ^EFG | ADFA Ddc^B | BEGB Ed^B=B |   ACEA CEA^A | BF F2 FAdc | B^ABc edcd | fga^a b=agf | edcB AGFE | D4 z B^df ||[K:B] bBdf aBdf |   gBdf gfdB | bBdf aBdf | gGce gecG | bGce aGce | gGce gecG | bGce aGce | gBdf gfdB | bBdf aBdf |   gBdf gfdB | g3 =a gfed | gfed cB^A=A | G=A^AB cBAB | def=g ^gfed | cBAG FEDC | [Bf]4 z Bdf |   bBdf aBdf | gBdf gfdB | bBdf aBdf | gGce gecG | bGce aGce | gGce gecG | bGce aGce | gBdf gfdB |   bBdf aBdf | gBdf gfdB | g3 =a gfed | gfed cB^A=A | G=A^AB cBAB | def=g ^gfed | cBAG FEDC |   B,3 F BF^EF ||[K:Bmin]\"^2nd time:\" [B,G]zF[B,B] z F^EF | [Ge]zF[ce] z F^EF | GFec ^AFE^D |   D3 ^A{c} B^A=A^G | GBeG BeGB | FBdF BdFd | c3 d cB^AB | c2 (3^ABc fF^EF | [B,G]zF[B,B] z F^EF |   [Ge]zF[ce] z F^EF | GFec ^AFE^D | D^CDF B^A=A^G | GFGB eGBe | FBdF Bd (3F^Ac | fFfg fedc |   [Bf]3 [Bf]2 F^EF | GF^EF BFEF | GF^EF cFEF | GFec ^AFE^D | D3 B{c} B^A=A^G | GBeG BeGB |   FBdF BdFd | ^EFcd cB^AB | c2 (3^ABc fF^EF | [B,G]zF[B,B] z F^EF | [Ge]zF[ce] z F^EF |   GFec ^AFE^D | D3 B{c} B^A=A^G | GFGB eGBe | FBdF Bd (3F^Ac | fFfg fedc | [Bf]3 [Bf]2 FG^G ||   [Af]z^G[Af] z G[Af]^A | [Be] z [Ge]2 z FG^G | ACEA CEc^B | B2 [FA]2 z [Fd][Fd]A | dAdA cAcA |   B2 G2 G=C^CE | czGB z FG^G | A z [Fd]2 z ^EFG | ADFA Dd^c=c | BEGB Ed (3c^B=B | ACEA CEA^A |   BF F2 FAdc | B^ABc edcd | fga^a b=agf | edcB AGFE | [Dd]3 D DFA^A | [Af]z^G[Af] z G[Af]^A |   [Be] z [Ge]2 z e^d=d | [cg]2 ^B [cg]2 B[cg]B | B2 [Ad]2 z [df][df]f | dAdA cAcA | B2 G2 G=C^CE |   czGB z FG^G | A z [Fd]2 z ^EFG | ADFA DFA^A | BEGB Ed (3c^B=B | ACEA CEA^A | BF F2 FAdc |   B^ABc edcd | fga^a b=agf | edcB AGFE | D4 z B^df ||[K:B] bBdf aBdf | gBdf gfdB | bBdf aBdf |   gGce gecG | bGce aGce | gGce gecG | bGce aGce | gBdf gfdB | bBdf aBdf | gBdf gfdB | g3 =a gfed |   gfed cB^A=A | G=A^AB cBAB | def=g ^gfed | cBAG FEDC | B3 B- Bzdf | bBdf aBdf | gBdf gfdB |   bBdf aBdf | gGce gecG | bGce aGce | gGce gecG | bGce aGce | gBdf gfdB | bagf edcB | AGFE DCB,=A, |   ^G,^B,DF GBdf | gfed cB^A=A | G3 B cBAB | def=g ^gfed | c2 (3BAG F2 (3EDC | B,4{c} B4 ||\n",
      "✅ Final unique token count: 78669\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Sort the dataset first\n",
    "# sorted_train_dataset = sorted(train_dataset, key=lambda x: len(x['abc notation']))\n",
    "\n",
    "# unique_tokens = set()\n",
    "\n",
    "# for tune in sorted_train_dataset:\n",
    "#     abc_raw = tune['abc notation']\n",
    "#     cleaned_abc = clean_abc_text(abc_raw)\n",
    "\n",
    "#     # Sanity check: skip long or empty lines\n",
    "#     if len(cleaned_abc) == 0 or len(cleaned_abc) > 2000:\n",
    "#         print(\"Long token: \", cleaned_abc)\n",
    "#         continue\n",
    "\n",
    "#     tokens = simple_tokenizer(cleaned_abc)\n",
    "\n",
    "#     # Only accept reasonably short tokens\n",
    "#     for token in tokens:\n",
    "#         if len(token) <= 15:\n",
    "#             unique_tokens.add(token)\n",
    "\n",
    "# print(f\"✅ Final unique token count: {len(unique_tokens)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca4f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c7da8ac",
   "metadata": {},
   "source": [
    "b) Model Implementation: Implement an RNN model (RNN Layer, LSTM Layer)using a deep learning framework of your choice (e.g. PyTorch).  \n",
    "\n",
    "The model should be able to take sequences of tokens as input and predict the next token in the sequence.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c7f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1abb0290",
   "metadata": {},
   "source": [
    "c) Training: Train your RNN model on the preprocessed dataset.  \n",
    "\n",
    "Experiment with different hyperparameters such as learning rate, batch size, and number of epochs to achieve the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb71ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad1833de",
   "metadata": {},
   "source": [
    "d) Music Generation: After training, use your RNN model to generate new music sequences.  \n",
    "\n",
    "You can start with a seed sequence and let the model predict subsequent tokens to create a complete tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2cfec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fcf7b7a",
   "metadata": {},
   "source": [
    "e) Evaluation: Evaluate the quality of the generated music. You can do this by listening to the tunes or by using metrics such as top-1 or top-5 accuracy.  \n",
    "\n",
    "You can also look for other metrics like BLEU score or perplexity.  \n",
    "\n",
    "Please use TensorBoard to visualize the training process and the generated music.  \n",
    "\n",
    "Here is a tutorial on how to use TensorBoard with PyTorch: https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/tensorboard_with_pytorch.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4061ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
