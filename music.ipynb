{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e88550",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "In this assignment, you will implement a Recurrent Neural Network (RNN) for music generation.  \n",
    "\n",
    "For this, you will use the Irish Massive ABC Notation (IrishMAN) dataset, which contains a collection of Irish folk tunes in ABC notation.  \n",
    "\n",
    "The goal is to train an RNN to generate new tunes based on the patterns learned from the dataset.\n",
    "\n",
    "**Dataset:**  \n",
    "IrishMAN Dataset can be found at [https://huggingface.co/datasets/sander-wood/irishman](https://huggingface.co/datasets/sander-wood/irishman).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b81747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data on cuda...\n",
      "Using 50000 tunes for training\n",
      "Vocab size: 95\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "# 0. Control the data size for quick testing\n",
    "SAMPLE_SIZE = 50000     # number of tunes to use (set to None for full dataset)\n",
    "\n",
    "# 1. Set parameters\n",
    "SEQ_LENGTH = 75       # number of characters per training sequence\n",
    "BATCH_SIZE = 32       # how many sequences per batch\n",
    "EMBED_DIM = 64        # size of character embeddings\n",
    "HIDDEN_DIM = 128      # size of LSTM hidden state\n",
    "NUM_LAYERS = 1        # number of LSTM layers\n",
    "LEARNING_RATE = 0.001 # optimizer learning rate\n",
    "EPOCHS = 20            # number of passes through the data\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 2. Load data from Hugging Face\n",
    "print(f\"Loading data on {DEVICE}...\")\n",
    "dataset = load_dataset('sander-wood/irishman', split='train')\n",
    "texts = dataset['abc notation']  # list of music strings\n",
    "if SAMPLE_SIZE is not None:\n",
    "    texts = texts[:SAMPLE_SIZE]   # keep only a small subset\n",
    "print(f\"Using {len(texts)} tunes for training\")\n",
    "\n",
    "# 3. Build a character-level vocabulary\n",
    "vocab = sorted(set(''.join(texts)))\n",
    "char2idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "idx2char = {i: ch for ch, i in char2idx.items()}\n",
    "VocabSize = len(vocab)\n",
    "print(f\"Vocab size: {VocabSize}\")\n",
    "\n",
    "# 4. Create a simple Dataset class\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.data = []\n",
    "        for t in texts:\n",
    "            # turn text into list of indices\n",
    "            seq = [char2idx.get(c, 0) for c in t]\n",
    "            # slice into pairs of (input, target)\n",
    "            for i in range(len(seq) - SEQ_LENGTH):\n",
    "                inp = seq[i:i+SEQ_LENGTH]\n",
    "                tgt = seq[i+1:i+SEQ_LENGTH+1]\n",
    "                self.data.append((inp, tgt))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        inp, tgt = self.data[idx]\n",
    "        return torch.tensor(inp), torch.tensor(tgt)\n",
    "\n",
    "# 5. Split into train/val\n",
    "random.shuffle(texts)\n",
    "split_idx = int(0.9 * len(texts))\n",
    "train_texts = texts[:split_idx]\n",
    "val_texts = texts[split_idx:]\n",
    "train_ds = MusicDataset(train_texts)\n",
    "val_ds = MusicDataset(val_texts)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 6. Define the LSTM model\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(VocabSize, EMBED_DIM)\n",
    "        self.lstm = nn.LSTM(EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_DIM, VocabSize)\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = out.reshape(-1, HIDDEN_DIM)\n",
    "        logits = self.fc(out)\n",
    "        return logits, hidden\n",
    "\n",
    "model = SimpleRNN().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 7. Training loop\n",
    "# Early stopping parameters\n",
    "patience = 5              # how many epochs to wait without improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE).view(-1)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(inputs)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE).view(-1)\n",
    "            logits, _ = model(inputs)\n",
    "            val_loss += criterion(logits, targets).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Check for improvement\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model here\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "            early_stop = True\n",
    "\n",
    "    if early_stop:\n",
    "        break\n",
    "# 8. Generate a sample sequence with dynamic M, L, K\n",
    "model.eval()\n",
    "# Keep X and T fixed, but choose M, L, K randomly\n",
    "M_options = [\"M:6/8\", \"M:4/4\", \"M:3/4\"]\n",
    "L_options = [\"L:1/8\", \"L:1/16\", \"L:1/4\"]\n",
    "K_options = [\"K:D\", \"K:G\", \"K:C\"]\n",
    "# Build header\n",
    "start = f\"\"\"X:1\n",
    "T:Generated Tune\n",
    "{random.choice(M_options)}\n",
    "{random.choice(L_options)}\n",
    "{random.choice(K_options)}\n",
    "\"\"\"\n",
    "# Convert header to indices\n",
    "seq = [char2idx.get(c, 0) for c in start]\n",
    "hidden = None\n",
    "# Generate additional 200 characters of music\n",
    "for _ in range(800):\n",
    "    inp_seq = seq[-SEQ_LENGTH:]\n",
    "    inp = torch.tensor(inp_seq).unsqueeze(0).to(DEVICE)\n",
    "    logits, hidden = model(inp, hidden)\n",
    "    probs = torch.softmax(logits[-1], dim=0)\n",
    "    idx = torch.multinomial(probs, 1).item()\n",
    "    seq.append(idx)\n",
    "\n",
    "print(\"Training finished in:\", datetime.now() - start_time)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, 'best_model.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert back to characters and print\n",
    "gen = ''.join(idx2char[i] for i in seq)\n",
    "print(\"Generated ABC notation with dynamic headers:\", gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054f965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
